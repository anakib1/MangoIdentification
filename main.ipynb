{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset of victorian era sentences of different authors. Downloads from huggingface in parquet format \n",
    "data = load_dataset('contemmcm/victorian_authorship')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegressionCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, top_k_accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Performs basic classification metrics for multi label classification. Return as dict to build dataframe later.\n",
    "def perform_metric(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=np.nan, average='macro'),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=np.nan, average='macro'),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=np.nan, average='macro')\n",
    "    }\n",
    "\n",
    "\n",
    "# Finds human-readable name for a model. KNN adds number of neighbours to name\n",
    "def get_name(model):\n",
    "    if isinstance(model, KNeighborsClassifier):\n",
    "        return f'{model.__class__.__name__}({str(model.n_neighbors)})'\n",
    "    return model.__class__.__name__\n",
    "\n",
    "\n",
    "#Runs train, evaluate pipeline on model zoo\n",
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    # Building zoo. Trying to reduce dimension by PCA with different number of components.\n",
    "    components_zoo = [None, 50, 400]\n",
    "    classifiers_zoo = [\n",
    "        RidgeClassifierCV(),\n",
    "        # Ridge classifier with cross-validation. Replaces classification task with regression one.  \n",
    "        make_pipeline(StandardScaler(), LogisticRegressionCV()),\n",
    "        # logistic regression. Works well only with scaled data.\n",
    "        KNeighborsClassifier(50),  #KNN\n",
    "        KNeighborsClassifier(100),  #KNN\n",
    "        RandomForestClassifier(n_estimators=50),  # Select 50 to try to avoid overfit\n",
    "        RadiusNeighborsClassifier(metric='cosine')  # Non-cosine works bad \n",
    "    ]\n",
    "    zoo = []\n",
    "    handles = []  # Names for index for resulting DF.\n",
    "\n",
    "    for component in components_zoo:\n",
    "        for model in classifiers_zoo:\n",
    "            if component is None:  # No PCA at all. \n",
    "                zoo.append(deepcopy(model))\n",
    "                handles.append(get_name(model))\n",
    "            else:\n",
    "                zoo.append(make_pipeline(PCA(component), model))  # Chain with PCA\n",
    "                handles.append('PCA+' + get_name(model))\n",
    "\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "\n",
    "    for model in tqdm(zoo[len(train_results):]):\n",
    "        model.fit(X_train, y_train)  # Not use fit_predict because not all classifiers support. \n",
    "        y_train_hat = model.predict(X_train)  # train preds\n",
    "        y_test_hat = model.predict(X_test)  # test preds\n",
    "\n",
    "        train_results.append(perform_metric(y_train, y_train_hat))\n",
    "        test_results.append(perform_metric(y_test, y_test_hat))\n",
    "\n",
    "    train_df = pd.DataFrame(train_results,\n",
    "                            index=handles[:len(train_results)])  # Build train df. For consistency truncate handles\n",
    "    test_df = pd.DataFrame(test_results, index=handles[:len(test_results)])  # Build test df\n",
    "\n",
    "    return train_df, test_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1bc47d50c5b2e5f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Will use weighted inverse frequencies as main feature\n",
    "\n",
    "def prepare_data(train_split, test_split):\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, strip_accents='ascii', stop_words='english')\n",
    "    # max features for speed reasons. remove accents for different encodings. remove stop words to potentially make features more useful \n",
    "    X_train, y_train = vectorizer.fit_transform(train_split['text']).toarray(), train_split['author']\n",
    "    # Will use already fitted vocab\n",
    "    X_test, y_test = vectorizer.transform(test_split['text']).toarray(), test_split['author']\n",
    "    # Assert shapes match\n",
    "    assert X_train.shape[1] == X_test.shape[1]\n",
    "    return X_train, y_train, X_test, y_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3408544997963c8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_data(data['train'].shuffle().select(range(10000)),\n",
    "                                                data['test'].shuffle().select(range(10000)))\n",
    "# Data is to big. Taking subset\n",
    "victorian_train, victorian_test = train_model(X_train, y_train, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af99039726a7a0c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "victorian_train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45f58774c7cb6897",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "victorian_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4e29c2e4d29443",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'Best test F1: {victorian_test[\"f1\"].max()}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d1c0b56115f27c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Blogs or comments. Should be harder \n",
    "blogs = load_dataset('night12/authorTextIdentification')\n",
    "# Renaming for consistency\n",
    "blogs = blogs.rename_column('author_id', 'author')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12d8f8a41b5f626f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_data(blogs['train'].shuffle().select(range(10000)),\n",
    "                                                blogs['validation'].shuffle().select(range(6000)))\n",
    "\n",
    "blogs_train, blogs_test = train_model(X_train, y_train, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d2cc15f2d24600b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "blogs_train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13a000ccc0439b2b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "blogs_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2beeec540983270",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'Best test F1: {blogs_test[\"f1\"].max()}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f317f2f650d4bafa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e387bf131bd11ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
